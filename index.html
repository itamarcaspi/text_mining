<!DOCTYPE html>
<html>
  <head>
    <title>Discussion of “Text Mining Methodologies with R”</title>
    <meta charset="utf-8">
    <meta name="author" content="Itamar Caspi" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="style\rutgers.css" type="text/css" />
    <link rel="stylesheet" href="style\rutgers-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Discussion of “Text Mining Methodologies with R”
## By Benchimol, Kazinnik, and Saadon
### Itamar Caspi
### December 30, 2018 (updated: 2018-12-26)

---




# What This Paper Does

- Reviews several existing methodologies in text analysis  

- Illustrates using R

- Presents potential applications related to central banking


---

# My Take From This Paper

- A great pedagogical service to the research department

- Brings attention to an important and relatively new tool

- Presents interesting, though preliminary, insights


---

# My Comments and Suggestions


- Need to improve the reproducibility of the paper

--

- Devote more care to the text preprocessing stage

--

- Increase transparency of the topic analysis exercise

---

class: inverse, center, middle

# Reproducable research:
&lt;img src="https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2016/howscientist.png" width="35%" /&gt;

---

# Issues with Reproducability

At current state, it is hard replicate the results in the paper since  

--

- Which packages where used other than **tm** and **tidytext**?  

--

- How was the data derived? Files? URLs? Where can we find it?

--

- There is no code available to reproduce the figures in the paper  

---

# Which Packages?

Reproducing the results depends on installing and loading the following packages:


```r
library(tm)       # for cleaning and transforming text to data
library(tidytext) # for working with tidy text objects
library(tidyverse)  # for data wrangling and generating plots
```
 


---

# Getting Data from the Web

The data are the raw material for this document, yet the document does not make it clear how to get them. It might be helpful to show how we can download some text using the **rvest** package and convert it to text using the **pdftools** package:


```r
library(rvest)
library(pdftools)
```
 

```r
pdf_urls &lt;- c("https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/November%2026%202018%20minutes.pdf",
         "https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/October%208%202018%20minutes.pdf")

minutes_url &lt;- pdf_urls %&gt;%
    map_chr(~ pdf_text(.) %&gt;%
              paste(collapse = " ") %&gt;% 
              gsub(pattern = "\r\n", replacement = " ") %&gt;% 
              tolower()
            )
```

PROBLEM: Minute documents' links don't have a well defined URL pattern. This makes it hard to loop over the entire archive.

---
# Get Data from Files

Alternatively, users can pre-download the PDF files prior the analysis and convert them to text files using the ***pdftools** package.  

First, make a list of the available files

```r
pdf_files &lt;- paste0("./documents/", list.files("./documents/"))
```

Next, convert the PDFs to text, clean some mess, and create a list of text files


```r
minutes_pdf &lt;- pdf_files %&gt;%
    map_chr(~ pdf_text(.) %&gt;%
              paste(collapse = " ") %&gt;% 
              gsub(pattern = "\r\n", replacement = " ") %&gt;% 
              tolower()
            )
```

---

# Get Document Dates

First, we need to load the **lubridate** package that handles date objects:


```r
library(lubridate)
```

The PDF files include information about their date of creation. We can use this to create a tibble with dates for every document
 

```r
minutes_dates &lt;- pdf_files %&gt;% 
  map(pdf_info) %&gt;% 
  map("created") %&gt;% 
  do.call("c", .) %&gt;% 
  as.tibble() %&gt;% 
  rename(date = value) %&gt;% 
  mutate(document = as.character(1:n()))

head(minutes_dates, 3)
```

```
## # A tibble: 3 x 2
##   date                document
##   &lt;dttm&gt;              &lt;chr&gt;   
## 1 2014-04-07 10:50:30 1       
## 2 2015-04-05 11:57:26 2       
## 3 2013-08-12 11:13:07 3
```

---

class: inverse, center, middle

# Preprocessing: 
![](https://imgs.xkcd.com/comics/machine_learning.png)

---

# Focus on What's Important

The important part of the minutes is "narrow-forum" discussion. The following loop cuts all of the text that is not included in the "narrow-forum" discussion


```r
for (i in 1:length(minutes_pdf)) {
  narrow_start &lt;- str_locate(minutes_pdf[i],
                             "the narrow-forum discussion")[1,1][[1]]
  narrow_end   &lt;- str_locate(minutes_pdf[i],
                             "participants in the narrow-forum discussion")[1,1][[1]] - 1
  minutes_pdf[i] &lt;- str_sub(minutes_pdf[i], narrow_start, narrow_end)
}
```

---

# Focus on What's Important

We can check that none of the list's elements is empty by sorting them out and examining their length:


```r
minutes_pdf %&gt;%
  map_chr(str_length) %&gt;%
  as.numeric() %&gt;%
  as_tibble() %&gt;%
  arrange(value) %&gt;% 
  head()
```

```
## # A tibble: 6 x 1
##   value
##   &lt;dbl&gt;
## 1     0
## 2  4166
## 3  5015
## 4  5145
## 5  5407
## 6  5424
```

---

# Cleaning the Text

The **tm** package enables us to "clean" the text - transform to lowercase letters, remove stop words and punctuation, etc.


```r
minutes_corpus &lt;- Corpus(VectorSource(minutes_pdf)) %&gt;% 
  tm_map(removeWords, stopwords("english")) %&gt;% 
  tm_map(removeNumbers) %&gt;% 
  tm_map(removePunctuation) %&gt;% 
  tm_map(stripWhitespace)  
  # tm_map(stemDocument)
```

Now we convert the list to a term document matrix 


```r
minutes_dtm &lt;- minutes_corpus %&gt;% 
  TermDocumentMatrix()

minutes_dtm
```

```
## &lt;&lt;TermDocumentMatrix (terms: 2567, documents: 100)&gt;&gt;
## Non-/sparse entries: 29128/227572
## Sparsity           : 89%
## Maximal term length: 22
## Weighting          : term frequency (tf)
```

---

# Working with Tidy Text

- In a tidy data set each variable is a column, each observation is a row, and each type of observational unit is an entry in the table.
- We can tidy the text using the `tidy()` function from the **tidytext** package.  
- Once the test is tidy we can easily use our dates tibble to add a date column with the date of each minutes  


```r
tidy_minutes &lt;- tidy(minutes_dtm) %&gt;% 
  rename(word = term) %&gt;% 
  left_join(minutes_dates)

head(tidy_minutes)
```

```
## # A tibble: 6 x 4
##   word         document count date               
##   &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt; &lt;dttm&gt;             
## 1 accompanied  1            1 2014-04-07 10:50:30
## 2 accompanying 1            1 2014-04-07 10:50:30
## 3 according    1            3 2014-04-07 10:50:30
## 4 achieve      1            1 2014-04-07 10:50:30
## 5 activity     1           11 2014-04-07 10:50:30
## 6 addition     1            1 2014-04-07 10:50:30
```

---

# Which Words Actually Matter?

Now, that the text is tidy, we can easily spot the most frequent words


```r
tidy_minutes_head &lt;- tidy_minutes %&gt;%
  group_by(word) %&gt;% 
  summarise(count_sum = sum(count)) %&gt;% 
  arrange(desc(count_sum)) %&gt;% 
  head()

tidy_minutes_head
```

```
## # A tibble: 6 x 2
##   word      count_sum
##   &lt;chr&gt;         &lt;dbl&gt;
## 1 rate           2300
## 2 interest       1327
## 3 inflation      1271
## 4 committee      1200
## 5 members        1032
## 6 growth          928
```

---

# Sentiment Analysis

The [Loughran and McDonald sentiment lexicon](https://sraf.nd.edu/textual-analysis/resources/) is induced in the __tidytext__ package


```r
get_sentiments("loughran") %&gt;%
    count(sentiment, sort = TRUE)
```

```
## # A tibble: 6 x 2
##   sentiment        n
##   &lt;chr&gt;        &lt;int&gt;
## 1 negative      2355
## 2 litigious      903
## 3 positive       354
## 4 uncertainty    297
## 5 constraining   184
## 6 superfluous     56
```

---

# Sentiment Analysis

Here is how we can apply the Loughran and McDonald sentiment analysis the "tidy" way


```r
minutes_sentiment &lt;- tidy_minutes %&gt;%
  mutate(year_pub = as.integer(year(date))) %&gt;% 
  add_count(date) %&gt;%
  rename(minutes_total = n) %&gt;% 
  inner_join(get_sentiments("loughran"))

head(minutes_sentiment)
```

```
## # A tibble: 6 x 7
##   word  document count date                year_pub minutes_total sentiment
##   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dttm&gt;                 &lt;int&gt;         &lt;int&gt; &lt;chr&gt;    
## 1 achi~ 1            1 2014-04-07 10:50:30     2014           364 positive 
## 2 appa~ 1            2 2014-04-07 10:50:30     2014           364 uncertai~
## 3 beli~ 1            2 2014-04-07 10:50:30     2014           364 uncertai~
## 4 bound 1            2 2014-04-07 10:50:30     2014           364 constrai~
## 5 decl~ 1            4 2014-04-07 10:50:30     2014           364 negative 
## 6 decl~ 1            2 2014-04-07 10:50:30     2014           364 negative
```

---

# Sentiment Analysis of BoI Minutes
## _Within_ Relative Frequency is More Informative

&lt;img src="index_files/figure-html/unnamed-chunk-18-1..svg" style="display: block; margin: auto;" /&gt;

---

# "Unsupervised" Topic Modeling

At its core, topic modelling is an unsupervised learning exercise. This leaves plenty of room for discretion and interpretation.  

Here is how its done:


```r
library(quanteda)
library(stm)

minutes_dfm &lt;- tidy_minutes %&gt;%
    count(document, word, sort = TRUE) %&gt;%
    cast_dfm(document, word, n)

*topic_model &lt;- stm(minutes_dfm, K = 6,
*                verbose = FALSE, init.type = "Spectral")
```

From the __stm__ manual:  
&lt;ru-blockquote&gt;"The most important user input in parametric topic models is the number of topics. There is no right answer to the appropriate number of topics. More topics will give more fine-grained representations of the data at the potential cost of being less precisely estimated."&lt;/ru-blockquote&gt;

---
# Which Words Contribute Most to  Each Topic?

&lt;img src="index_files/figure-html/unnamed-chunk-20-1..svg" style="display: block; margin: auto;" /&gt;

---

# The Distrubution of Topics Across Documents

&lt;img src="index_files/figure-html/unnamed-chunk-21-1..svg" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle

# Miscellaneous

---

# Minor Comments

- Provide an RMarkdown version of the document  

- Avoid using word clouds  

- Check robustness w.r.t `stemDocument`

---

class: inverse, center, middle

# Thanks!

Comments?

\begin{tabular}{rr}
[&lt;i class="fa fa-paper-plane"&gt;&lt;/i&gt; ](mailto:caspi.itamar@gmail.com) &amp; caspi.itamar@gmail.com \\
[&lt;i class="fa fa-github"&gt;&lt;/i&gt; ](https://github.com/itamarcaspi/) &amp; github.com/itamarcaspi
\end{tabular}`


[&lt;i class="fa fa-paper-plane"&gt;&lt;/i&gt; ](mailto:caspi.itamar@gmail.com) caspi.itamar@gmail.com \|
[&lt;i class="fa fa-github"&gt;&lt;/i&gt; ](https://github.com/itamarcaspi/) github.com/itamarcaspi
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">   <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">   </div> </div> "
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
