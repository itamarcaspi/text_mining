<!DOCTYPE html>
<html>
  <head>
    <title>Discussion of “Text Mining Methodologies with R”</title>
    <meta charset="utf-8">
    <meta name="author" content="Itamar Caspi" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="style\rutgers.css" type="text/css" />
    <link rel="stylesheet" href="style\rutgers-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Discussion of “Text Mining Methodologies with R”
## By Benchimol, Kazinnik, and Saadon
### Itamar Caspi
### December 30, 2018 (updated: 2018-12-27)

---




# What This Paper Does

- Reviews several existing methodologies in text analysis  

- Illustrates using R

- Presents potential applications related to central banking


---

# My Take From This Paper

- A great pedagogical service to the research department

- Brings attention to an important and relatively new tool

- Presents interesting, though preliminary, insights


---

# My Comments and Suggestions


- Need to improve the reproducibility of the paper

--

- Devote more care to the text preprocessing stage

--

- Increase transparency of the topic analysis exercise

---

class: inverse, center, middle

# Reproducable research:
&lt;img src="https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2016/howscientist.png" width="35%" /&gt;

---

# Issues with Reproducability

At current state, it is hard replicate the results in the paper since  

--

- Which packages where used other than **tm** and **tidytext**?  

--

- How was the data derived? Files? URLs? Where can we find it?

--

- There is no code available to reproduce the figures in the paper  

---

# Which Packages?

Reproducing the results depends on installing and loading the following packages:


```r
library(tm)       # for cleaning and transforming text to data
library(tidytext) # for working with tidy text objects
library(tidyverse)  # for data wrangling and visualization
```
 


---

# Getting Data from the Web

The data are the raw material for this document, yet the document does not make it clear how to get them. It might be helpful to show how we can download some text using the **rvest** package and convert it to text using the **pdftools** package:


```r
library(rvest)
library(pdftools)
```
 

```r
pdf_urls &lt;- c("https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/November%2026%202018%20minutes.pdf",
         "https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/October%208%202018%20minutes.pdf")

minutes_url &lt;- pdf_urls %&gt;%
    map_chr(~ pdf_text(.) %&gt;%
              paste(collapse = " ") %&gt;% 
              gsub(pattern = "\r\n", replacement = " ") %&gt;% 
              tolower()
            )
```

PROBLEM: Minute documents' links don't have a well defined URL pattern. This makes it hard to loop over the entire archive.

---
# Get Data from Files

Alternatively, users can pre-download the PDF files prior the analysis and convert them to text files using the ***pdftools** package.  

First, make a list of the available files

```r
pdf_files &lt;- paste0("./documents/", list.files("./documents/"))
```

Next, convert the PDFs to text, clean some mess, and create a list of text files


```r
minutes_pdf &lt;- pdf_files %&gt;%
    map_chr(~ pdf_text(.) %&gt;%
              paste(collapse = " ") %&gt;% 
              gsub(pattern = "\r\n", replacement = " ") %&gt;%
              tolower()
            )
```

---

# Get Document Dates

First, we need to load the **lubridate** package that handles date objects:


```r
library(lubridate)
```

The PDF files include information about their date of creation. We can use this to create a tibble with dates for every document
 

```r
minutes_dates &lt;- pdf_files %&gt;% 
  map(pdf_info) %&gt;% 
  map("created") %&gt;% 
  do.call("c", .) %&gt;% 
  as.tibble() %&gt;% 
  rename(date = value) %&gt;% 
  mutate(document = row_number())

head(minutes_dates, 3)
```

```
## # A tibble: 3 x 2
##   date                document
##   &lt;dttm&gt;                 &lt;int&gt;
## 1 2014-04-07 10:50:30        1
## 2 2015-04-05 11:57:26        2
## 3 2013-08-12 11:13:07        3
```

---

class: inverse, center, middle

# Preprocessing: 
![](https://imgs.xkcd.com/comics/machine_learning.png)

---

# Focus on What's Important

The important part of the minutes is "narrow-forum" discussion. The following loop cuts all of the text that is not included in the "narrow-forum" discussion


```r
for (i in 1:length(minutes_pdf)) {
  narrow_start &lt;- str_locate(minutes_pdf[i],
                             "the narrow-forum discussion")[1,1][[1]]
  narrow_end   &lt;- str_locate(minutes_pdf[i],
                             "participants in the narrow-forum discussion")[1,1][[1]] - 1
  minutes_pdf[i] &lt;- str_sub(minutes_pdf[i], narrow_start, narrow_end)
}
```

---

# Focus on What's Important

We can check that none of the list's elements is empty by sorting them out and examining their length:


```r
minutes_pdf %&gt;%
  map_int(str_length) %&gt;%
  as_tibble() %&gt;% 
  rename(word_count = value) %&gt;% 
  mutate(document = row_number()) %&gt;% 
  left_join(minutes_dates) %&gt;% 
  arrange(word_count) %&gt;% 
  head() %&gt;% 
  kable(format = "html")
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; word_count &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; document &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; date &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2015-07-06 12:59:22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4166 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 38 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2017-12-11 08:52:52 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5015 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 43 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2017-11-02 13:02:11 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5145 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 47 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2018-06-10 16:03:07 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5407 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 37 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2018-01-24 12:12:02 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5424 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 77 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2012-10-09 15:56:14 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Tidying the Text

- In a tidy data set each variable is a column, each observation is a row, and each type of observational unit is an entry in the table.
- We can tidy the text using the `unest_tokens()` function from the **tidytext** package.  
- Once the text is tidy we can easily use our `minutes_date` tibble to add a date column with the date of each minutes  


```r
tidy_minutes &lt;- minutes_pdf %&gt;% 
    as_tibble() %&gt;% 
    mutate(document = row_number()) %&gt;% 
    mutate(value = str_split(value,"\r\n")) %&gt;% 
    unnest(value) %&gt;% 
    unnest_tokens(word, value) %&gt;% 
    filter(str_detect(word, "[a-z]")) %&gt;%
    anti_join(stop_words) %&gt;% 
    left_join(minutes_dates) %&gt;% 
    filter(date &gt;= ymd("2011-11-7"))
    

head(tidy_minutes)
```

```
## # A tibble: 6 x 3
##   document word       date               
##      &lt;int&gt; &lt;chr&gt;      &lt;dttm&gt;             
## 1        1 narrow     2014-04-07 10:50:30
## 2        1 forum      2014-04-07 10:50:30
## 3        1 discussion 2014-04-07 10:50:30
## 4        1 rate       2014-04-07 10:50:30
## 5        1 decision   2014-04-07 10:50:30
## 6        1 april      2014-04-07 10:50:30
```


---

# Which Words Actually Matter?

Now, we can easily answer what are the most common words in these documents

```r
tidy_minutes %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  top_n(5)
```

```
## # A tibble: 5 x 2
##   word          n
##   &lt;chr&gt;     &lt;int&gt;
## 1 rate       1989
## 2 committee  1194
## 3 inflation  1057
## 4 growth      816
## 5 percent     620
```


```r
tidy_minutes &lt;- tidy_minutes %&gt;% 
  filter(!word %in% c("rate", "committee"))
```

---

# Sentiment Analysis

The [Loughran and McDonald sentiment lexicon](https://sraf.nd.edu/textual-analysis/resources/) is induced in the __tidytext__ package


```r
get_sentiments("loughran") %&gt;%
    count(sentiment, sort = TRUE)
```

```
## # A tibble: 6 x 2
##   sentiment        n
##   &lt;chr&gt;        &lt;int&gt;
## 1 negative      2355
## 2 litigious      903
## 3 positive       354
## 4 uncertainty    297
## 5 constraining   184
## 6 superfluous     56
```

---

# Sentiment Analysis

Here is how we can apply the Loughran and McDonald sentiment analysis the "tidy" way


```r
minutes_sentiment &lt;- tidy_minutes %&gt;%
  add_count(date) %&gt;%
  rename(minutes_total = n) %&gt;% 
  inner_join(get_sentiments("loughran"))

head(minutes_sentiment)
```

```
## # A tibble: 6 x 5
##   document word        date                minutes_total sentiment   
##      &lt;int&gt; &lt;chr&gt;       &lt;dttm&gt;                      &lt;int&gt; &lt;chr&gt;       
## 1        1 declined    2014-04-07 10:50:30           644 negative    
## 2        1 bound       2014-04-07 10:50:30           644 constraining
## 3        1 bound       2014-04-07 10:50:30           644 constraining
## 4        1 stability   2014-04-07 10:50:30           644 positive    
## 5        1 decline     2014-04-07 10:50:30           644 negative    
## 6        1 improvement 2014-04-07 10:50:30           644 positive
```

---

# Sentiment Analysis of BoI Minutes
## __Within__ Relative Frequency is More Informative

&lt;img src="index_files/figure-html/unnamed-chunk-17-1..svg" style="display: block; margin: auto;" /&gt;

---

# "Unsupervised" Topic Modeling

At its core, topic modelling is an unsupervised learning exercise. This leaves plenty of room for discretion and interpretation.  

Here is how its done:


```r
library(quanteda)
library(stm)

minutes_dfm &lt;- tidy_minutes %&gt;%
    count(document, word, sort = TRUE) %&gt;%
    cast_dfm(document, word, n)

*topic_model &lt;- stm(minutes_dfm, K = 6,
*                verbose = FALSE, init.type = "Spectral")
```

From the __stm__ manual:  
&lt;ru-blockquote&gt;"The most important user input in parametric topic models is the number of topics. There is no right answer to the appropriate number of topics. More topics will give more fine-grained representations of the data at the potential cost of being less precisely estimated."&lt;/ru-blockquote&gt;

---
# Which Words Contribute Most to  Each Topic?

&lt;img src="index_files/figure-html/unnamed-chunk-19-1..svg" style="display: block; margin: auto;" /&gt;

---

# The Distrubution of Topics Across Documents

&lt;img src="index_files/figure-html/unnamed-chunk-20-1..svg" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle

# Miscellaneous

---

# Minor Comments

- Provide an RMarkdown version of the document  

- Avoid using word clouds  

- Check robustness w.r.t `stemDocument`

---

class: inverse, center, middle

# Thanks!

Comments?

[&lt;i class="fa fa-paper-plane"&gt;&lt;/i&gt; ](mailto:caspi.itamar@gmail.com) caspi.itamar@gmail.com \|
[&lt;i class="fa fa-github"&gt;&lt;/i&gt; ](https://github.com/itamarcaspi/) github.com/itamarcaspi  



---

# Appendix

---

# Cleaning the Text

The **tm** package enables us to "clean" the text - transform to lowercase letters, remove stop words and punctuation, etc.


```r
minutes_corpus &lt;- Corpus(VectorSource(minutes_pdf)) %&gt;% 
  tm_map(removeWords, stopwords("english")) %&gt;% 
  tm_map(removeNumbers) %&gt;% 
  tm_map(removePunctuation) %&gt;% 
  tm_map(stripWhitespace)  
  # tm_map(stemDocument)
```

Now we convert the list to a term document matrix 


```r
minutes_dtm &lt;- minutes_corpus %&gt;% 
  TermDocumentMatrix()

minutes_dtm
```

---



```r
minutes_keep &lt;- minutes_pdf %&gt;% 
  as_tibble() %&gt;% 
  mutate(is_keep = ifelse(str_detect(value, "decided to keep"), 1, 0)) %&gt;% 
  select(is_keep) %&gt;% 
  mutate(document = row_number())

df &lt;- minutes_pdf %&gt;% 
  as_tibble() %&gt;% 
  mutate(document = row_number()) %&gt;% 
  mutate(value = str_split(value,"\r\n")) %&gt;% 
  rename(line = value) %&gt;% 
  left_join(minutes_dates) %&gt;% 
  left_join(minutes_keep) %&gt;% 
  unnest(line)
```


```r
library(rsample)

minutes_split &lt;- df %&gt;%
  select(document) %&gt;%
  initial_split()
train_data &lt;- training(minutes_split)
test_data &lt;- testing(minutes_split)
```



```r
sparse_words &lt;- tidy_minutes %&gt;%
  count(document, word) %&gt;%
  inner_join(train_data) %&gt;%
  cast_sparse(document, word, n)

class(sparse_words)
dim(sparse_words)
```
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "<div class=\"progress-bar-container\">   <div class=\"progress-bar\" style=\"width: calc(%current% / %total% * 100%);\">   </div> </div> "
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
