---
title: "Discussion of"
subtitle: "\"Communication and Transparency through Central Banks Texts\" <br/>__and__<br/>\"Text Mining Methodologies with R\"<br/>__by__<br/>Benchimol, Kazinnik, and Saadon"
author: "Itamar Caspi"
date: "Bank of Israel  <br/> December 30, 2018 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/middlebury.css", "style/middlebury-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      
---

```{r setup, include=FALSE}
library(svglite)
library(knitr)
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      dev = "svglite",
                      fig.ext = ".svg")

options(htmltools.dir.version = FALSE)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

# What These Papers Do

- Reviews several existing methodologies in text analysis. 

- Illustrates using R.

- Presents applications related to central bank transparency.


---

# My Take 

These papers

  - provide a great pedagogical service to the research department.
  
  - bring attention to an important and relatively new tool.
  
  - assemble a novel dataset on international central bank announcements.
  
  - present many interesting insights which relate to monetary policy.


---

# My Comments


- Need to improve the __reproducibility__ of the methodological paper.

--

- Devote more care to the text __preprocessing__ stage.

--

- Focus on a __specific__ application (Forecasting or Transparency).

---

class: title-slide-section-blue, center, middle

# Reproducable research
```{r, echo=FALSE, eval=FALSE, out.width = "35%"}
include_graphics("https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2016/howscientist.png")
```

---

# Issues with Reproducability

At current state, it is hard replicate the results in the paper since it is hard to tell  

--

- Which packages where used other than **tm** and **tidytext**?  

--

- How was the data derived? Files? URLs? Where can we find it?

--

- What is the code to used to produce some of the figures in the text?  

---

# Which Packages?

Reproducing the results in the paper depends on installing and loading the following packages:

```{r}

library(tm)         # for cleaning and transforming text to data
library(tidytext)   # for working with tidy text objects
library(tidyverse)  # for wrangling date and visualization

```
 
```{r, echo=FALSE}
theme_set(theme_light())  # set default theme for all plots
```

---

# How to Get Data from the Web

The data are the raw material for this document, yet the document does not make it clear how to get them. It might be helpful to show how we can download some text using the **rvest** package and convert it to text using the **pdftools** package:

```{r}
library(rvest)
library(pdftools)
```
 
```{r eval=FALSE}

pdf_urls <- c("https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/November%2026%202018%20minutes.pdf",
         "https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/October%208%202018%20minutes.pdf")

minutes_url <- pdf_urls %>%
    map_chr(~ pdf_text(.) %>%
              paste(collapse = " ") %>% 
              gsub(pattern = "\r\n", replacement = " ") %>% 
              tolower()
            )

```

PROBLEM: Minutes links don't have a well defined URL pattern. This makes it hard to loop over the entire archive.

---
# How to Get Data from Files

Alternatively, users can pre-download the PDF files prior the analysis and convert them to text files using the **pdftools** package.  

First, make a list of the available files
```{r}
pdf_files <- paste0("./documents/", list.files("./documents/"))

```

Next, convert the PDFs to text, clean some mess, and create a list of text files

```{r}
minutes_pdf <- pdf_files %>%
    map_chr(~ pdf_text(.) %>%
              paste(collapse = " ") %>% 
              gsub(pattern = "\r\n", replacement = " ") %>%
              tolower()
            )
```

---

# How to Get Document Dates

The PDF files include information about their date of creation. We can use this to create a tibble with dates for every document
 
```{r}
library(lubridate) # for working with dates and times.

minutes_dates <- pdf_files %>% 
  map(pdf_info) %>% 
  map("created") %>% 
  do.call("c", .) %>% 
  as.tibble() %>% 
  rename(date = value) %>% 
  mutate(document = row_number())

head(minutes_dates)
```

---

class: title-slide-section-blue, center, middle

# Preprocessing
![](https://imgs.xkcd.com/comics/machine_learning.png)

---

# Focus on What's Important

The important part of the minutes is "narrow-forum" discussion. The following loop cuts all of the text that is not included in the "narrow-forum" discussion

```{r }
for (i in 1:length(minutes_pdf)) {
  narrow_start <- str_locate(minutes_pdf[i],
                             "the narrow-forum discussion")[1,1][[1]]
  narrow_end   <- str_locate(minutes_pdf[i],
                             "participants in the narrow-forum discussion")[1,1][[1]] - 1
  minutes_pdf[i] <- str_sub(minutes_pdf[i], narrow_start, narrow_end)
}
```

---

# Focus on What's Important

We can check that none of the list's elements is empty by sorting them out and examining their length:

```{r}
minutes_pdf %>%
  map_int(str_length) %>%
  as_tibble() %>% 
  rename(word_count = value) %>% 
  mutate(document = row_number()) %>% 
  left_join(minutes_dates) %>% 
  arrange(word_count) %>% 
  head() %>% 
  kable(format = "html")
  
```

---

# Tidying the Text

- In a tidy data set each variable is a column, each observation is a row, and each type of observational unit is a table.
- We can tidy the text using the `unest_tokens()` function from the **tidytext** package.

```{r }

tidy_minutes <- minutes_pdf %>% 
    as_tibble() %>% 
    mutate(document = row_number()) %>% 
    mutate(value = str_split(value,"\r\n")) %>% 
    unnest(value) %>% 
    unnest_tokens(word, value) %>% 
    filter(str_detect(word, "[a-z]")) %>%
    anti_join(stop_words) %>% 
    left_join(minutes_dates) %>% 
    filter(date >= ymd("2011-11-7"))
    
head(tidy_minutes, 5)
```


---

# Which Words Actually Matter?

Now that the text is tidy, we can easily answer questions like "what are the most common words in these documents?"
```{r}
tidy_minutes %>% 
  count(word, sort = TRUE) %>% 
  top_n(5)
  
```

We might want to remove uninformative(?) words

```{r }

tidy_minutes <- tidy_minutes %>% 
  filter(!word %in% c("rate", "committee"))

```

---

# Sentiment Analysis

The [Loughran and McDonald sentiment lexicon](https://sraf.nd.edu/textual-analysis/resources/) is induced in the __tidytext__ package

```{r}
get_sentiments("loughran") %>%
    count(sentiment, sort = TRUE)
```

---

# Sentiment Analysis within Tidytext

Here is how we can apply the Loughran and McDonald sentiment analysis the "tidy" way

```{r }

minutes_sentiment <- tidy_minutes %>%
  add_count(date) %>%
  rename(minutes_total = n) %>% 
  inner_join(get_sentiments("loughran"))

head(minutes_sentiment)

```

---

# Sentiment Analysis of BoI Minutes
## __Within__ Relative Frequency Might Be More Informative

```{r echo = FALSE, fig.width=10, fig.height=6, fig.align='center'}

minutes_sentiment %>%
    count(date, minutes_total, sentiment) %>%
    filter(sentiment %in% c("positive", "negative", 
                            "uncertainty")) %>%
    mutate(sentiment = factor(sentiment, levels = c("negative",
                                                    "positive",
                                                    "uncertainty")
                                                    )) %>%
    ggplot(aes(date, n / minutes_total, color = sentiment)) +
    geom_line() +
    labs(y = "Relative frequency", x = NULL)

```

---

# "Unsupervised" Topic Modeling

At its core, topic modelling is an unsupervised learning exercise. This leaves plenty of room for discretion and interpretation.  

Here is how its done:

```{r, eval=TRUE }

library(quanteda)
library(stm)

minutes_dfm <- tidy_minutes %>%
    count(document, word, sort = TRUE) %>%
    cast_dfm(document, word, n)

topic_model <- stm(minutes_dfm, K = 6, #<<
                 verbose = FALSE, init.type = "Spectral") #<<

```

From the __stm__ manual:  
<midd-blockquote>"The most important user input in parametric topic models is the number of topics. There is no right answer to the appropriate number of topics. More topics will give more fine-grained representations of the data at the potential cost of being less precisely estimated."</midd-blockquote>


---

# The Distrubution of Topics Across Documents is Informative

```{r echo=FALSE, fig.width=10, fig.height=6, fig.align='center'}

td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(minutes_dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 3) +
  labs(y = "Number of minutes",
       x = expression(gamma)) +
  theme_minimal()

```
---

class: title-slide-section-blue, center, middle

# One Paper One Idea

---

# Which Way to Go?

- In my view, the paper has three contribution
  
  1. Generates a rich, international data sets on central bank announcements.
  2. Provides evidence on interest rate predictability.
  3. Suggests novel measures for transparency.

- Unfortunately, the latter two are currently presented as "Applications".  

- I recommend highlighting results and data rather than methodologies.

- __One idea, one paper__: I think the authors must choose between forecasting and transparency indicators.
---

# Redefining the Forecasting Exercise

The forecasting exercise can be formulated as a **multi-task learning** problem, e.g., 

$$\mbox{Pr}(Y_t=k|X_{t-1}=x)=\frac{e^{\beta_{0k}+\beta_k^Tx}}{\sum_{\ell=1}^Ke^{\beta_{0\ell}+\beta_\ell^Tx}}.$$

where $Y_t$ is the direction of change in the interest rate
$$k\in\{\text{increase,no-change,decrease}\},$$  

and $X_{t-1}$ is a vector of predetermined features which includes a battery of text-related indicators, as well as other economic and financial variables and __interactions__ terms.  

IMPORTANT: The goal should be defined in terms of **prediction quality** and not $p$-values or $R^2$.  

---

# Regularized Multinomial Logit

The Lasso seems extremely fit to answer these types of questions, mostly because it is easy to interpret.  

Multinomial Lasso model can be easily estimated using the __glmnet__ package, e.g.,

```{r, eval=FALSE }
library(glmnet)

fit = glmnet(X, Y, family = "multinomial", type.multinomial = "grouped")

```


This will hopefully enable us to answer interesting questions such as:

  - Does adding text-related indicators __outpreform__ a naive model?  
  
  - Which indicators (or interactions) __matter__ for prediction?  
  
  - Are there __cross-country__ effects?


This framework can easily extend itself to predict other response variables (exchange rates, the yield curve, etc.).

CAVEAT: Temporal dependence.

---

class: title-slide-section-blue, center, middle

# Miscellaneous

---

# A Few More Suggestions

- Provide an RMarkdown version of the methodological document.  

- Add more part-of-speech features using the [**cleanNLP**](https://statsmaths.github.io/cleanNLP/) R package.

- Check robustness w.r.t `stemDocument` and `stopWords`.

- Avoid word clouds.

- Refer to Gentzkow, M., Kelly, B. T., & Taddy, M. (2017). Text as data (No. w23276). National Bureau of Economic Research.

- Sentiment and topics as dependent variables? (Hansen, _et al._ 2017, QJE).


---

class: inverse, center, middle

# Thanks!

[<i class="fa fa-paper-plane"></i> ](mailto:caspi.itamar@gmail.com) caspi.itamar@gmail.com \|
[<i class="fa fa-github"></i> ](https://github.com/itamarcaspi/) github.com/itamarcaspi  


.footnote[
Prepared using [RMarkdown](https://rmarkdown.rstudio.com/) and [xaringan](https://github.com/yihui/xaringan). The source code of this presentation can be found [here](https://github.com/itamarcaspi/text_mining). 
]
<!-- --- -->

<!-- # Appendix -->

<!-- --- -->

<!-- # Which Words Contribute to Which Topics? -->

<!-- ```{r , eval= FALSE, echo=FALSE, fig.width=10, fig.height=6, fig.align='center'} -->
<!-- # devtools::install_github('dgrtwo/drlib') -->
<!-- library(drlib) -->

<!-- td_beta <- tidy(topic_model) -->

<!-- td_beta %>% -->
<!--     group_by(topic) %>% -->
<!--     top_n(10, beta) %>% -->
<!--     ungroup() %>% -->
<!--     mutate(topic = paste0("Topic ", topic), -->
<!--            term = reorder_within(term, beta, topic)) %>% -->
<!--     ggplot(aes(term, beta, fill = as.factor(topic))) + -->
<!--     geom_col(alpha = 0.8, show.legend = FALSE) + -->
<!--     facet_wrap(~ topic, scales = "free_y") + -->
<!--     coord_flip() + -->
<!--     scale_x_reordered() + -->
<!--     labs(x = NULL, y = expression(beta)) + -->
<!--     theme_minimal() -->

<!-- ``` -->


<!-- --- -->
<!-- # Cleaning the Text -->

<!-- The **tm** package enables us to "clean" the text - transform to lowercase letters, remove stop words and punctuation, etc. -->

<!-- ```{r, eval=FALSE} -->
<!-- minutes_corpus <- Corpus(VectorSource(minutes_pdf)) %>%  -->
<!--   tm_map(removeWords, stopwords("english")) %>%  -->
<!--   tm_map(removeNumbers) %>%  -->
<!--   tm_map(removePunctuation) %>%  -->
<!--   tm_map(stripWhitespace)   -->
<!--   # tm_map(stemDocument) -->

<!-- ``` -->

<!-- Now we convert the list to a term document matrix  -->

<!-- ```{r, eval=FALSE} -->
<!-- minutes_dtm <- minutes_corpus %>%  -->
<!--   TermDocumentMatrix() -->

<!-- minutes_dtm -->
<!-- ``` -->

<!-- --- -->
<!-- # LASSO -->

<!-- ```{r, eval=FALSE} -->

<!-- minutes_keep <- minutes_pdf %>%  -->
<!--   as_tibble() %>%  -->
<!--   mutate(is_keep = ifelse(str_detect(value, "decided to keep"), 1, 0)) %>%  -->
<!--   select(is_keep) %>%  -->
<!--   mutate(document = row_number()) -->

<!-- df <- minutes_pdf %>%  -->
<!--   as_tibble() %>%  -->
<!--   mutate(document = row_number()) %>%  -->
<!--   mutate(value = str_split(value,"\r\n")) %>%  -->
<!--   rename(line = value) %>%  -->
<!--   left_join(minutes_dates) %>%  -->
<!--   left_join(minutes_keep) %>%  -->
<!--   unnest(line) -->

<!-- ``` -->

<!-- ```{r, eval=FALSE} -->

<!-- library(rsample) -->

<!-- minutes_split <- df %>% -->
<!--   select(document) %>% -->
<!--   initial_split() -->
<!-- train_data <- training(minutes_split) -->
<!-- test_data <- testing(minutes_split) -->

<!-- ``` -->


<!-- ```{r, eval=FALSE} -->

<!-- sparse_words <- tidy_minutes %>% -->
<!--   count(document, word) %>% -->
<!--   inner_join(train_data) %>% -->
<!--   cast_sparse(document, word, n) -->

<!-- class(sparse_words) -->
<!-- dim(sparse_words) -->
<!-- ``` -->
