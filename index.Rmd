---
title: "Discussion of \"Text Mining Methodologies with R\""
subtitle: "By Benchimol, Kazinnik, and Saadon"
author: "Itamar Caspi"
date: "December 30, 2018 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/rutgers.css", "style/rutgers-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      
---

```{r setup, include=FALSE}
library(svglite)
library(knitr)
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      dev = "svglite",
                      fig.ext = ".svg")

options(htmltools.dir.version = FALSE)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

# What This Paper Does

- Reviews several existing methodologies in text analysis  

- Illustrates using R

- Presents potential applications related to central banking


---

# My Take From This Paper

- A great pedagogical service to the research department

- Brings attention to an important and relatively new tool

- Presents interesting, though preliminary, insights


---

# My Comments and Suggestions


- Need to improve the reproducibility of the paper

--

- Devote more care to the text preprocessing stage

--

- Increase transparency of the topic analysis exercise

---

class: inverse, center, middle

# Reproducable research:
```{r, echo=FALSE, out.width = "35%"}
include_graphics("https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2016/howscientist.png")
```

---

# Issues with Reproducability

At current state, it is hard replicate the results in the paper since  

--

- Which packages where used other than **tm** and **tidytext**?  

--

- How was the data derived? Files? URLs? Where can we find it?

--

- There is no code available to reproduce the figures in the paper  

---

# Which Packages?

Reproducing the results depends on installing and loading the following packages:

```{r}

library(tm)       # for cleaning and transforming text to data
library(tidytext) # for working with tidy text objects
library(tidyverse)  # for data wrangling and generating plots

```
 
```{r, echo=FALSE}
theme_set(theme_light())  # set default theme for all plots
```

---

# Getting Data from the Web

The data are the raw material for this document, yet the document does not make it clear how to get them. It might be helpful to show how we can download some text using the **rvest** package and convert it to text using the **pdftools** package:

```{r}
library(rvest)
library(pdftools)
```
 
```{r eval=FALSE}

pdf_urls <- c("https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/November%2026%202018%20minutes.pdf",
         "https://www.boi.org.il/en/NewsAndPublications/RegularPublications/Protocols/October%208%202018%20minutes.pdf")

minutes_url <- pdf_urls %>%
    map_chr(~ pdf_text(.) %>%
              paste(collapse = " ") %>% 
              gsub(pattern = "\r\n", replacement = " ") %>% 
              tolower()
            )

```

PROBLEM: Minute documents' links don't have a well defined URL pattern. This makes it hard to loop over the entire archive.

---
# Get Data from Files

Alternatively, users can pre-download the PDF files prior the analysis and convert them to text files using the ***pdftools** package.  

First, make a list of the available files
```{r}
pdf_files <- paste0("./documents/", list.files("./documents/"))

```

Next, convert the PDFs to text, clean some mess, and create a list of text files

```{r}
minutes_pdf <- pdf_files %>%
    map_chr(~ pdf_text(.) %>%
              paste(collapse = " ") %>% 
              # gsub(pattern = "\r\n", replacement = " ") %>%
              tolower()
            )

```

---

# Get Document Dates

First, we need to load the **lubridate** package that handles date objects:

```{r }

library(lubridate)

```

The PDF files include information about their date of creation. We can use this to create a tibble with dates for every document
 
```{r}

minutes_dates <- pdf_files %>% 
  map(pdf_info) %>% 
  map("created") %>% 
  do.call("c", .) %>% 
  as.tibble() %>% 
  rename(date = value) %>% 
  mutate(document = row_number())

head(minutes_dates, 3)

```

---

class: inverse, center, middle

# Preprocessing: 
![](https://imgs.xkcd.com/comics/machine_learning.png)

---

# Focus on What's Important

The important part of the minutes is "narrow-forum" discussion. The following loop cuts all of the text that is not included in the "narrow-forum" discussion

```{r }
for (i in 1:length(minutes_pdf)) {
  narrow_start <- str_locate(minutes_pdf[i],
                             "the narrow-forum")[1,1][[1]]
  narrow_end   <- str_locate(minutes_pdf[i],
                             "participants in the narrow-forum discussion")[1,1][[1]] - 1
  minutes_pdf[i] <- str_sub(minutes_pdf[i], narrow_start, narrow_end)
}
```

---

# Focus on What's Important

We can check that none of the list's elements is empty by sorting them out and examining their length:

```{r}
minutes_pdf %>%
  map_chr(str_length) %>%
  as.numeric() %>%
  as_tibble() %>%
  arrange(value) %>% 
  head()
```

---

# Cleaning the Text

The **tm** package enables us to "clean" the text - transform to lowercase letters, remove stop words and punctuation, etc.

```{r}
minutes_corpus <- Corpus(VectorSource(minutes_pdf)) %>% 
  tm_map(removeWords, stopwords("english")) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(stripWhitespace)  
  # tm_map(stemDocument)

```

Now we convert the list to a term document matrix 

```{r}
minutes_dtm <- minutes_corpus %>% 
  TermDocumentMatrix()

minutes_dtm
```

---

# Working with Tidy Text

- In a tidy data set each variable is a column, each observation is a row, and each type of observational unit is an entry in the table.
- We can tidy the text using the `tidy()` function from the **tidytext** package.  
- Once the test is tidy we can easily use our dates tibble to add a date column with the date of each minutes  

```{r}
tidy_minutes <- tidy(minutes_dtm) %>% 
  mutate(document = as.integer(document)) %>% 
  rename(word = term) %>% 
  left_join(minutes_dates)

head(tidy_minutes)
```

---

# Which Words Actually Matter?

Now, that the text is tidy, we can easily spot the most frequent words

```{r}
tidy_minutes_head <- tidy_minutes %>%
  group_by(word) %>% 
  summarise(count_sum = sum(count)) %>% 
  arrange(desc(count_sum)) %>% 
  head()

tidy_minutes_head
  
```

---

# Sentiment Analysis

The [Loughran and McDonald sentiment lexicon](https://sraf.nd.edu/textual-analysis/resources/) is induced in the __tidytext__ package

```{r}
get_sentiments("loughran") %>%
    count(sentiment, sort = TRUE)
```

---

# Sentiment Analysis

Here is how we can apply the Loughran and McDonald sentiment analysis the "tidy" way

```{r }

minutes_sentiment <- tidy_minutes %>%
  mutate(year_pub = as.integer(year(date))) %>% 
  add_count(date) %>%
  rename(minutes_total = n) %>% 
  inner_join(get_sentiments("loughran"))

head(minutes_sentiment)

```

---

# Sentiment Analysis of BoI Minutes
## _Within_ Relative Frequency is More Informative

```{r echo = FALSE, fig.width=10, fig.height=6, fig.align='center'}

minutes_sentiment %>%
    count(date, minutes_total, sentiment) %>%
    filter(sentiment %in% c("positive", "negative", 
                            "uncertainty")) %>%
    mutate(sentiment = factor(sentiment, levels = c("negative",
                                                    "positive",
                                                    "uncertainty")
                                                    )) %>%
    ggplot(aes(date, n / minutes_total, color = sentiment)) +
    geom_line() +
    labs(y = "Relative frequency", x = NULL)

```

---

# "Unsupervised" Topic Modeling

At its core, topic modelling is an unsupervised learning exercise. This leaves plenty of room for discretion and interpretation.  

Here is how its done:

```{r }

library(quanteda)
library(stm)

minutes_dfm <- tidy_minutes %>%
    count(document, word, sort = TRUE) %>%
    cast_dfm(document, word, n)

topic_model <- stm(minutes_dfm, K = 6, #<<
                 verbose = FALSE, init.type = "Spectral") #<<

```

From the __stm__ manual:  
<ru-blockquote>"The most important user input in parametric topic models is the number of topics. There is no right answer to the appropriate number of topics. More topics will give more fine-grained representations of the data at the potential cost of being less precisely estimated."</ru-blockquote>

---
# Which Words Contribute Most to  Each Topic?

```{r echo=FALSE, fig.width=10, fig.height=6, fig.align='center'}
# devtools::install_github('dgrtwo/drlib')
library(drlib)

td_beta <- tidy(topic_model)

td_beta %>%
    group_by(topic) %>%
    top_n(8, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta)) +
    theme_minimal()

```

---

# The Distrubution of Topics Across Documents

```{r echo=FALSE, fig.width=10, fig.height=6, fig.align='center'}

td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(minutes_dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 3) +
  labs(y = "Number of minutes",
       x = expression(gamma)) +
  theme_minimal()

```

---

class: inverse, center, middle

# Miscellaneous

---

# Minor Comments

- Provide an RMarkdown version of the document  

- Avoid using word clouds  

- Check robustness w.r.t `stemDocument`

---

class: inverse, center, middle

# Thanks!

Comments?

\begin{tabular}{rr}
[<i class="fa fa-paper-plane"></i> ](mailto:caspi.itamar@gmail.com) & caspi.itamar@gmail.com \\
[<i class="fa fa-github"></i> ](https://github.com/itamarcaspi/) & github.com/itamarcaspi
\end{tabular}


[<i class="fa fa-paper-plane"></i> ](mailto:caspi.itamar@gmail.com) caspi.itamar@gmail.com \|
[<i class="fa fa-github"></i> ](https://github.com/itamarcaspi/) github.com/itamarcaspi  



---

# Appendix


```{r }

minutes_keep <- minutes_pdf %>% 
  as_tibble() %>% 
  mutate(is_keep = ifelse(str_detect(value, "decided to keep"), 1, 0)) %>% 
  select(is_keep) %>% 
  mutate(document = row_number())

df <- minutes_pdf %>% 
  as_tibble() %>% 
  mutate(document = row_number()) %>% 
  mutate(value = str_split(value,"\r\n")) %>% 
  rename(line = value) %>% 
  left_join(minutes_dates) %>% 
  left_join(minutes_keep) %>% 
  unnest(line)
  
```

```{r remedy001}

library(rsample)

minutes_split <- df %>%
  select(document) %>%
  initial_split()
train_data <- training(minutes_split)
test_data <- testing(minutes_split)

```


```{r remedy002}

sparse_words <- tidy_minutes %>%
  count(document, word) %>%
  inner_join(train_data) %>%
  cast_sparse(document, word, n)

class(sparse_words)
dim(sparse_words)
```
